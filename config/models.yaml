# LLM Proxifier Model Configuration
#
# This configuration runs in ON-DEMAND mode by default (on_demand_only: true).
# In on-demand mode:
# - Models start automatically when first requested (cold start may take 1-3 minutes)
# - Models shut down after timeout_minutes of inactivity (default: 5 minutes)
# - The auto_start and preload settings are ignored
# - All configured models appear as "available" in /v1/models endpoint
#
# To revert to legacy auto-start behavior, set environment variable:
# ON_DEMAND_ONLY=false
#
# Model Configuration Fields:
# - port: Local port for the model server
# - model_path: Path to the GGUF model file
# - context_length: Maximum context window size
# - gpu_layers: Number of layers to offload to GPU (-1 = all)
# - auto_start: [IGNORED in on-demand mode] Start model on server startup
# - preload: [IGNORED in on-demand mode] Keep model always loaded
# - priority: Loading priority (higher = loaded first when concurrent limit hit)
# - resource_group: Logical grouping for batch operations
# - additional_args: Extra arguments passed to llama.cpp server

models:
  qwen-32b:
    port: 11001
    model_path: "~/models/qwen-32b/qwen2.5-coder-32b-instruct-q8_0.gguf"
    context_length: 32768
    gpu_layers: -1
    auto_start: false
    preload: false
    priority: 1
    resource_group: "large-models"
    additional_args:
      - "-b 2048"
      - "-t 8"

  deepseek-lite:
    port: 11002
    model_path: "~/models/deepseek-lite/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf"
    context_length: 16384
    gpu_layers: -1
    auto_start: true
    preload: true
    priority: 10
    resource_group: "lite-models"
    additional_args:
      - "-b 4096"
      - "-t 8"

  qwen-7b:
    port: 11003
    model_path: "~/models/qwen-7b/qwen2.5-coder-7b-instruct-q5_k_m.gguf"
    context_length: 32768
    gpu_layers: -1
    auto_start: true
    preload: false
    priority: 5
    resource_group: "medium-models"
    additional_args:
      - "-b 3072"
      - "-t 8"

  llama-70b:
    port: 11004
    model_path: "~/models/llama-70b/meta-llama_Llama-4-Scout-17B-16E-Instruct-Q8_0-00001-of-00003.gguf"
    context_length: 8192
    gpu_layers: 30
    auto_start: false
    preload: false
    priority: 1
    resource_group: "large-models"
    additional_args:
      - "-b 2048"
      - "-t 16"
