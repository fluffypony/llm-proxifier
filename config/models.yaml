# LLM Proxifier Model Configuration
#
# This configuration runs in ON-DEMAND mode by default (on_demand_only: true).
# In on-demand mode:
# - Models start automatically when first requested (cold start may take 1-3 minutes)
# - Models shut down after timeout_minutes of inactivity (default: 5 minutes)
# - The auto_start and preload settings are ignored
# - All configured models appear as "available" in /v1/models endpoint
#
# To revert to legacy auto-start behavior, use: llm-proxifier start --no-on-demand
#
# Model Configuration Fields:
# - port: Local port for the model server
# - model_path: Path to the GGUF model file
# - priority: Loading priority (higher = loaded first when concurrent limit hit)
# - resource_group: Logical grouping for batch operations
# - additional_args: ALL arguments passed to llama-server (except --model and --port)
#                   Can use: ["-c 16384", "-ngl -1"] or ["-c", "16384", "-ngl", "-1"]
#                   Note: --host defaults to 127.0.0.1 for security, override with "--host 0.0.0.0"

models:
  qwen-32b:
    port: 11001
    model_path: "~/models/qwen-32b/qwen2.5-coder-32b-instruct-q8_0.gguf"
    auto_start: false
    preload: false
    priority: 1
    resource_group: "large-models"
    additional_args:
      - "-c 65536"
      - "-ngl -1"
      - "-b 2048"
      - "-t 8"

  deepseek-lite:
    port: 11002
    model_path: "~/models/deepseek-lite/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf"
    auto_start: true
    preload: true
    priority: 10
    resource_group: "lite-models"
    additional_args:
      - "-c 131072"
      - "-ngl -1"
      - "-b 4096"
      - "-t 8"

  qwen-7b:
    port: 11003
    model_path: "~/models/qwen-7b/qwen2.5-coder-7b-instruct-q5_k_m.gguf"
    auto_start: true
    preload: false
    priority: 5
    resource_group: "medium-models"
    additional_args:
      - "-c 131072"
      - "-ngl -1"
      - "-b 3072"
      - "-t 8"

  llama-70b:
    port: 11004
    model_path: "~/models/llama-70b/meta-llama_Llama-4-Scout-17B-16E-Instruct-Q8_0-00001-of-00003.gguf"
    auto_start: false
    preload: false
    priority: 1
    resource_group: "large-models"
    additional_args:
      - "-c 16384"
      - "-ngl 30"
      - "-b 2048"
      - "-t 16"
